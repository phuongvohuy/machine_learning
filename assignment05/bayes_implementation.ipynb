{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "#define constant\n",
    "labelAConstant = \"zclass_A\"\n",
    "labelBConstant = \"zclass_B\"\n",
    "labelEConstant = \"zclass_E\"\n",
    "labelVConstant = \"zclass_V\"\n",
    "\n",
    "#constant used in model dataframe\n",
    "totalLabel = \"total\"\n",
    "\n",
    "CONSTANT_LABEL_ABSTRACT = \"abstract\"\n",
    "CONSTANT_LABEL_CLASS = \"class\"\n",
    "\n",
    "classNameList = [labelAConstant, labelBConstant, labelEConstant, labelVConstant]\n",
    "\n",
    "# Display the first 10 rows\n",
    "#text_df = pd.read_csv(\"./trg.csv\")\n",
    "#text_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Retrieve worlds from abstract. Manipulate words if required\n",
    "\"\"\"\n",
    "def retrieveWordListFromAbstract(abstract):\n",
    "    stopwords = [\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"]\n",
    "    redudancyWords = [\"IS\", \"A\", \"THE\", \"OF\", \"AND\", \"IN\", \"TO\"]\n",
    "    words = [x.strip().lower() for x in abstract.split(\" \")]\n",
    "    words = [ x for x in words if x != \"\"] \n",
    "    words = [ x for x in words if not(x in stopwords)] \n",
    "    \n",
    "    return words\n",
    "\n",
    "\"\"\"\n",
    "Create default dictionary data for featureList\n",
    "return a dictionary with key is feature and value is 0\n",
    "\"\"\" \n",
    "def createDefaultDictionaryFromFeatureList(featureList):\n",
    "    result = {}\n",
    "    for feature in featureList:\n",
    "        result[feature] = 0\n",
    "    return result    \n",
    "\n",
    "\"\"\"\n",
    "Map abstractData to an dictionary which has key is feature \n",
    "and value = 1 if that feature appear in abstractData, otherwise value = 0\n",
    "\"\"\"\n",
    "#transform each abstract data to attribute list\n",
    "def transformData(abstractData, featureList):\n",
    "    words_abstract = retrieveWordListFromAbstract(abstractData)\n",
    "    unique_words = np.unique(words_abstract)\n",
    "    \n",
    "    #create default value for dictionary list\n",
    "    dictionaryResult = createDefaultDictionaryFromFeatureList(featureList)\n",
    "    \n",
    "    # assign the feature which appear in abstract data\n",
    "    for index in range(0, len(unique_words)):\n",
    "        word = unique_words[index]\n",
    "        foundInList = word in featureList\n",
    "        if foundInList:\n",
    "            dictionaryResult[word] = 1            \n",
    "    \n",
    "    return dictionaryResult;\n",
    "\n",
    "def transformDataForMNBC(abstractData, featureList):\n",
    "    words_abstract = retrieveWordListFromAbstract(abstractData)\n",
    "    #unique_words = np.unique(words_abstract)\n",
    "    #get occurence of words in the list \n",
    "    common_words = Counter(words_abstract).most_common()\n",
    "    \n",
    "    #create default value for dictionary list\n",
    "    dictionaryResult = createDefaultDictionaryFromFeatureList(featureList)\n",
    "    \n",
    "    # assign the feature which appear in abstract data\n",
    "    #for index in range(0, len(unique_words)):\n",
    "    for index in range(0, len(common_words)):\n",
    "        #word = unique_words[index]\n",
    "        word = common_words[index][0]\n",
    "        foundInList = word in featureList\n",
    "        if foundInList:\n",
    "            dictionaryResult[word] = common_words[index][1]            \n",
    "    \n",
    "    return dictionaryResult;\n",
    "\n",
    "\"\"\"\n",
    "Create featureList by composing the 1000 most frequency keywords in abstractDataList\n",
    "\"\"\"\n",
    "def get1000MostFrequencyFeatureFrom(abstractDataList):\n",
    "    words = []\n",
    "    for index in range(0, abstractDataList.size):\n",
    "        abstract = abstractDataList[index]\n",
    "        words_abstract = retrieveWordListFromAbstract(abstract)\n",
    "        words.extend(words_abstract)\n",
    "    occurenceList = Counter(words)\n",
    "    \n",
    "    #get 1000 most common objects with their occurences\n",
    "    #TODO temparorily use 10\n",
    "    #most_1000_frequency = occurenceList.most_common(1000)\n",
    "    most_1000_frequency = occurenceList.most_common(100)\n",
    "    \n",
    "    features = np.empty(len(most_1000_frequency), dtype=object)\n",
    "    #features = []\n",
    "    # withraw all the keywords\n",
    "    for index in range(0, len(most_1000_frequency)):\n",
    "        features[index] = most_1000_frequency[index][0]\n",
    "        #features.append(most_1000_frequency[index][0])\n",
    "    \n",
    "    #return features\n",
    "    return features.tolist()\n",
    "\n",
    "\"\"\"\n",
    "Create featureList by composing all words which appear in abstractList\n",
    "\"\"\"\n",
    "def retrieveFeatureFrom(abstractDataList):\n",
    "    words = []\n",
    "    for index in range(0, abstractDataList.size):\n",
    "        abstract = abstractDataList[index]\n",
    "        words_abstract = retrieveWordListFromAbstract(abstract)\n",
    "        words.extend(words_abstract)\n",
    "\n",
    "    words = list(np.sort(np.unique(words)))\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "# Create dataset for training from featureList and abstractList\n",
    "def buildTrainingDataset(featureList, abstractList):\n",
    "    dataFrame = {}\n",
    "    aLength = len(abstractList)\n",
    "    fLength = len(featureList)\n",
    "    \n",
    "    #create default list\n",
    "    for featureIndex in range(0, fLength):\n",
    "        featureName = featureList[featureIndex]\n",
    "        dataFrame[featureName] = []\n",
    "        #dataFrame[featureName]= np.empty(aLength, dtype=object)\n",
    "        \n",
    "    for index in range(0, aLength):\n",
    "        abstract = abstractList[index]\n",
    "        #dictionaryForAbstract = transformData(abstract, featureList)\n",
    "        #TODO: temporarily used for MNBC\n",
    "        dictionaryForAbstract = transformDataForMNBC(abstract, featureList)\n",
    "        \n",
    "        for typleFeatureItem in dictionaryForAbstract.items():\n",
    "            featureName = typleFeatureItem[0]\n",
    "            featureValue = typleFeatureItem[1]\n",
    "            \n",
    "            dataFrame[featureName].append(featureValue)\n",
    "            \n",
    "    return dataFrame\n",
    "\n",
    "\n",
    "def buildTestingDataset(featureList, abstractList):\n",
    "    dataFrame = {}\n",
    "    aLength = len(abstractList)\n",
    "    fLength = len(featureList)\n",
    "    \n",
    "    #create default list\n",
    "    for featureIndex in range(0, fLength):\n",
    "        featureName = featureList[featureIndex]\n",
    "        dataFrame[featureName] = []\n",
    "        #dataFrame[featureName]= np.empty(aLength, dtype=object)\n",
    "        \n",
    "    for index in range(0, aLength):\n",
    "        abstract = abstractList[index]\n",
    "        #dictionaryForAbstract = transformData(abstract, featureList)\n",
    "        #TODO: temporarily used for MNBC\n",
    "        dictionaryForAbstract = transformDataForMNBC(abstract, featureList)\n",
    "        \n",
    "        for typleFeatureItem in dictionaryForAbstract.items():\n",
    "            featureName = typleFeatureItem[0]\n",
    "            featureValue = typleFeatureItem[1]\n",
    "            \n",
    "            dataFrame[featureName].append(featureValue)\n",
    "            \n",
    "    return dataFrame\n",
    "\n",
    "def buildClass(classList):\n",
    "    uniqueClassList = np.unique(classList)\n",
    "    noClass = len(classList)\n",
    "    resultValue = {}\n",
    "    \n",
    "    #assign default value\n",
    "    for classValue in uniqueClassList:\n",
    "        tempList = np.zeros(noClass, dtype = int)\n",
    "        resultValue[\"zclass_\"+classValue] = tempList.tolist()\n",
    "        \n",
    "    #assign the real value for resultValue\n",
    "    for index in range(0, noClass):\n",
    "        classValue = classList[index]\n",
    "        resultValue[\"zclass_\" + classValue][index] = 1\n",
    "    return resultValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def countFeatureAndClassByValue(featureName, featureValue, className, classValue, featureDF, classDF):\n",
    "    nRows = featureDF.shape[1]\n",
    "    count = 0\n",
    "    for index in range(0, nRows):\n",
    "        fValue = featureDF[featureName][index]\n",
    "        cValue = classDF[className][index]\n",
    "        if fValue == featureValue and cValue == classValue:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# Find the probability of featureName = featureValue given className = 1 with featuresDF and classDF\n",
    "# featureValue = 1 or 0\n",
    "def calculateProbabilityOf(featureName, featureValue, className, classValue, trainDF):\n",
    "    noClass = np.count_nonzero(trainDF[className])\n",
    "    condition = (trainDF[featureName] == featureValue) & (trainDF[className] == classValue)\n",
    "    count = trainDF[condition].shape[0]\n",
    "    \n",
    "    return count/noClass;\n",
    "    \n",
    "\n",
    "def retrieveClassHasValueAtIndex(index, classDF):\n",
    "    rows = classDF.iloc[index]\n",
    "    columnNames = classDF.columns.values\n",
    "    for className in columnNames:\n",
    "        if rows[className] == 1:\n",
    "            return className\n",
    "    return \"aa\"\n",
    "\n",
    "# retrieve total number of all the class\n",
    "def getTotalInModel(model):\n",
    "    totalSeries = model.loc[totalLabel]\n",
    "    return totalSeries.sum()\n",
    "\n",
    "def findMaxIndexInList(list):\n",
    "    return list.index(max(list))\n",
    "    \n",
    "# train\n",
    "def train(featuresDF, classDF):\n",
    "    #calculate probability of each class\n",
    "    nTotal = featuresDF.shape[0]\n",
    "\n",
    "    # calculate\n",
    "    classList = classDF.columns.values\n",
    "    tempList = list(map(lambda x: [x + \"=0\", x + \"=1\"], classList))\n",
    "    columnsNameList = [item for sublist in tempList for item in sublist]\n",
    "    \n",
    "    #calculate indexList\n",
    "    featureList = featuresDF.columns.values\n",
    "    tempList = list(map(lambda x: [x + \"=0\", x + \"=1\"], featureList))\n",
    "    indexList = [item for sublist in tempList for item in sublist]\n",
    "    #indexList.append(totalLabel)\n",
    "    \n",
    "\n",
    "    #compose the target model\n",
    "    result = np.zeros((len(indexList), len(columnsNameList)), dtype = int)\n",
    "    resultDF = pd.DataFrame(result , columns = columnsNameList, index=indexList)\n",
    "    \n",
    "    for rowIndex in range(0, nTotal):\n",
    "        #get active class for this row\n",
    "        #activeClassAtIndex = retrieveClassHasValueAtIndex(rowIndex, classDF)\n",
    "        \n",
    "        #calculate value for feature \n",
    "        for featureName in featureList:\n",
    "            featureValue = featuresDF[featureName][rowIndex]\n",
    "            rowName = featureName + \"=\" + str(featureValue)\n",
    "            for className in classList:\n",
    "                classValue = classDF[className][rowIndex]\n",
    "                columnName = className + \"=\" + str(classValue)\n",
    "                resultDF[columnName][rowName] += 1 \n",
    "            \n",
    "    resultDF = resultDF + 1 # increase all occurence by 1 to avoid multiple zero\n",
    "    \n",
    "    #calculate total for class\n",
    "    #for className in columnsNameList:\n",
    "    #    count = classDF[(classDF[className] == 1)].shape[0]\n",
    "    #    #each class have category for each feature. so when increasing occurence of feature by 1, we should increase total by 2\n",
    "    #    resultDF[className][totalLabel] = count + 2 \n",
    "    \n",
    "    #return a model\n",
    "    return resultDF\n",
    "\n",
    "def predict(testFeatureDF, classList, model):\n",
    "    result = np.zeros((len(testFeatureDF), len(classList)), dtype = int)\n",
    "    resultDF = pd.DataFrame(result , columns = classList)\n",
    "    \n",
    "    testLen = testFeatureDF.shape[0]\n",
    "    \n",
    "    for index in range(0, testLen):\n",
    "        instance = testFeatureDF.loc[index]\n",
    "        classPrediction = predictInstance(instance, classList, model)\n",
    "        \n",
    "        #assign to resultDF\n",
    "        for className in classList:\n",
    "            resultDF[className][index] = classPrediction[className]\n",
    "        \n",
    "    return resultDF\n",
    "\n",
    "# make prediction for instance base on model\n",
    "# instance is Series object of pandas, and retrieved from the test dataframe\n",
    "def predictInstance(instance, classList, model):\n",
    "    classLen = len(classList)\n",
    "    featureLen = len(instance)\n",
    "    #totalNo = getTotalInModel(model)\n",
    "    totalNo = model.sum().sum()\n",
    "    featureList = instance.index.tolist()\n",
    "    \n",
    "    #print(\"instance\", instance)\n",
    "    \n",
    "    probabilitiesOfFeatureForClass = np.zeros(classLen, dtype = float).tolist()\n",
    "    for indexClass in range(0, classLen):\n",
    "        className = classList[indexClass]\n",
    "        classValue = className + \"=1\" #calculate probability for class=1\n",
    "        #retrieve total number of class\n",
    "        noClass = model[classValue].sum()\n",
    "        probabilityOfClass = math.log(noClass) - math.log(totalNo)\n",
    "        \n",
    "        #retrieve probability of instance given by className\n",
    "        productOfCountFeature = 0\n",
    "        \n",
    "        for featureName in featureList:\n",
    "            featureVal = instance[featureName]\n",
    "            modelRows = featureName + \"=\" + str(featureVal)\n",
    "            productOfCountFeature = productOfCountFeature + math.log(model[classValue][modelRows]) - math.log(totalNo)\n",
    "        \n",
    "        #update probability of instance given class\n",
    "        probabilitiesOfFeatureForClass[indexClass] = productOfCountFeature - (featureLen-1)*probabilityOfClass\n",
    "    \n",
    "    \n",
    "    #print(\"probabilitiesOfFeatureForClass\", probabilitiesOfFeatureForClass)\n",
    "    maxIndex = findMaxIndexInList(probabilitiesOfFeatureForClass)\n",
    "    \n",
    "    #compose the result\n",
    "    result = {}\n",
    "    for indexClass in range(0, classLen):\n",
    "        className = classList[indexClass]\n",
    "        result[className] = 0\n",
    "        if maxIndex == indexClass:\n",
    "            result[className] = 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "#calculate accuracy of predicted data\n",
    "def calculateAccuracy(predictedDF, originalDF):\n",
    "    totalNo = originalDF.shape[0]\n",
    "    correctNo = 0\n",
    "    for rowIndex in range(0, totalNo):\n",
    "        originalRowItem = originalDF.loc[rowIndex]\n",
    "        predictedRowItem = predictedDF.loc[rowIndex]\n",
    "        if originalRowItem.equals(predictedRowItem):\n",
    "            correctNo += 1\n",
    "    \n",
    "    #print(\"totalNo \", totalNo)\n",
    "    #print(\"correctNo \", correctNo)\n",
    "    return correctNo / totalNo\n",
    "\n",
    "def cross_validation_split(dataset, foldValue):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / foldValue)\n",
    "    for i in range(foldValue):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# doing cross validating for trainingDF\n",
    "def kCrossValidate(trainingDF, classList, foldValue):\n",
    "    featureDF = trainingDF.drop(classList, axis=1)\n",
    "    classDF = trainingDF[classList]\n",
    "    \n",
    "    #create index list to make validation split\n",
    "    data = list(range(trainingDF.shape[0]))\n",
    "    foldIndexList = cross_validation_split(data, foldValue)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for index in range(0, len(foldIndexList)):\n",
    "        foldIndexItemList = foldIndexList[index]\n",
    "        trainingFeatureDF = featureDF.drop(foldIndexItemList)\n",
    "        trainingFeatureDF.index = range(trainingFeatureDF.shape[0])\n",
    "        trainingClassDF = classDF.drop(foldIndexItemList)\n",
    "        trainingClassDF.index = range(trainingClassDF.shape[0])\n",
    "        \n",
    "        #get testing data\n",
    "        testFeatureDF = featureDF.loc[foldIndexItemList]\n",
    "        testFeatureDF.index = range(testFeatureDF.shape[0])\n",
    "        testClassDF = classDF.loc[foldIndexItemList]\n",
    "        testClassDF.index = range(testClassDF.shape[0])\n",
    "        \n",
    "        #create model\n",
    "        #model = train(trainingFeatureDF, trainingClassDF)\n",
    "        model = trainMNBC(trainingFeatureDF, trainingClassDF)\n",
    "        \n",
    "        #prediction = predict(testFeatureDF, classNameList, model)\n",
    "        prediction = predictMNBC(testFeatureDF, classNameList, model)\n",
    "        accuracy = calculateAccuracy(prediction, testClassDF)\n",
    "        \n",
    "        print(\"accuracy \", accuracy)\n",
    "        \n",
    "        outputItem = {\n",
    "            \"model\" : model,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "        \n",
    "        result.append(outputItem)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# utilities to retrieve information from model training list\n",
    "def retrieve_mean_accuracy(outputs):\n",
    "    totalAccuracy = 0\n",
    "    for index in range(0, len(outputs)):\n",
    "        resultItem = outputs[index]\n",
    "        totalAccuracy += resultItem[\"accuracy\"]\n",
    "    averageAccuracy = totalAccuracy / len(outputs)\n",
    "    return averageAccuracy\n",
    "\n",
    "def retrieve_model_have_max_accuracy(outputs):\n",
    "    accuracyList = list(map(lambda x: x[\"accuracy\"], outputs))\n",
    "    indexMax = accuracyList.index(max(accuracyList))\n",
    "    return outputs[indexMax][\"model\"]\n",
    "\n",
    "def generate_prediction_output(predictionDF):\n",
    "    result = {\n",
    "        \"id\": list(range(predictionDF.shape[0])),\n",
    "        \"class\": []\n",
    "    }\n",
    "    for index in range(0, predictionDF.shape[0]):\n",
    "        rowItem = predictionDF.loc[index]\n",
    "        classList = rowItem.index.tolist()\n",
    "        valueList = rowItem.values.tolist()\n",
    "        maxIndex = valueList.index(max(valueList))\n",
    "        \n",
    "        if classList[maxIndex] == labelAConstant:\n",
    "            predictedValue = \"A\"\n",
    "        elif classList[maxIndex] == labelBConstant:\n",
    "            predictedValue = \"B\"\n",
    "        elif classList[maxIndex] == labelEConstant:\n",
    "            predictedValue = \"E\"\n",
    "        else:\n",
    "            predictedValue = \"V\"\n",
    "        result[\"class\"].append(predictedValue)\n",
    "        \n",
    "    return pd.DataFrame(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for Multinomial NBC\n",
    "# train\n",
    "def trainMNBC(featuresDF, classDF):\n",
    "    #calculate probability of each class\n",
    "    nTotal = featuresDF.shape[0]\n",
    "\n",
    "    # calculate\n",
    "    classList = classDF.columns.values.tolist()\n",
    "    \n",
    "    #calculate indexList\n",
    "    featureList = featuresDF.columns.values.tolist()\n",
    "    indexList = featureList.copy()\n",
    "    indexList.append(totalLabel)\n",
    "    \n",
    "    #compose the target model\n",
    "    result = np.zeros((len(indexList), len(classList)), dtype = int)\n",
    "    resultDF = pd.DataFrame(result , columns = classList, index=indexList)\n",
    "    \n",
    "    for rowIndex in range(0, nTotal):\n",
    "        #get active class for this row\n",
    "        activeClassName = retrieveClassHasValueAtIndex(rowIndex, classDF)\n",
    "        \n",
    "        #calculate value for feature \n",
    "        for featureName in featureList:\n",
    "            featureValue = featuresDF[featureName][rowIndex]\n",
    "            resultDF[activeClassName][featureName] += featureValue\n",
    "            \n",
    "    resultDF = resultDF + 1 # increase all occurence by 1 to avoid multiple zero\n",
    "    \n",
    "    #calculate total for class\n",
    "    for className in classList:\n",
    "        count = classDF[(classDF[className] == 1)].shape[0]\n",
    "        #each class have category for each feature. so when increasing occurence of feature by 1, we should increase total by 2\n",
    "        resultDF[className][totalLabel] = count\n",
    "    \n",
    "    return resultDF\n",
    "\n",
    "# predict MNBC\n",
    "def predictMNBC(testFeatureDF, classList, model):\n",
    "    result = np.zeros((len(testFeatureDF), len(classList)), dtype = int)\n",
    "    resultDF = pd.DataFrame(result , columns = classList)\n",
    "    \n",
    "    testLen = testFeatureDF.shape[0]\n",
    "    \n",
    "    for index in range(0, testLen):\n",
    "        instance = testFeatureDF.loc[index]\n",
    "        classPrediction = predictInstanceMNBC(instance, classList, model)\n",
    "        \n",
    "        #assign to resultDF\n",
    "        for className in classList:\n",
    "            resultDF[className][index] = classPrediction[className]\n",
    "        \n",
    "    return resultDF\n",
    "\n",
    "# make prediction for instance base on model\n",
    "# instance is Series object of pandas, and retrieved from the test dataframe\n",
    "# For MNBC\n",
    "def predictInstanceMNBC(instance, classList, model):\n",
    "    classLen = len(classList)\n",
    "    featureLen = len(instance)\n",
    "    noAllClass = model.loc[\"total\"].sum()\n",
    "    featureList = instance.index.tolist()\n",
    "        \n",
    "    probabilitiesOfFeatureForClass = np.zeros(classLen, dtype = float).tolist()\n",
    "    for indexClass in range(0, classLen):\n",
    "        className = classList[indexClass]\n",
    "        #retrieve total number of class\n",
    "        noOfClass = model[className][\"total\"]\n",
    "        probabilityOfClass = noOfClass / noAllClass\n",
    "        \n",
    "        totalClassByFeature = model[className].sum() - noOfClass\n",
    "        \n",
    "        #retrieve probability of instance given by className\n",
    "        productOfCountFeature = probabilityOfClass\n",
    "        #print(\"productOfCountFeature \", productOfCountFeature)\n",
    "        #print(\"totalClassByFeature \", totalClassByFeature)\n",
    "        \n",
    "        for featureName in featureList:\n",
    "            featureOccurence = instance[featureName]\n",
    "            #print(\"featureValue\", model[className][featureName])\n",
    "            #print(\"featureName\", featureName)\n",
    "            if featureOccurence > 0:\n",
    "                #print(\"featureName\", featureName)\n",
    "                #print(\"occurence\", model[className][featureName])\n",
    "                productOfCountFeature = productOfCountFeature + featureOccurence*math.log(model[className][featureName]/totalClassByFeature)\n",
    "        \n",
    "        #update probability of instance given class\n",
    "        probabilitiesOfFeatureForClass[indexClass] = productOfCountFeature\n",
    "    \n",
    "    \n",
    "    #print(\"probabilitiesOfFeatureForClass\", probabilitiesOfFeatureForClass)\n",
    "    maxIndex = findMaxIndexInList(probabilitiesOfFeatureForClass)\n",
    "    #print(\"probabilitiesOfFeatureForClass \", probabilitiesOfFeatureForClass)\n",
    "    \n",
    "    #compose the result\n",
    "    result = {}\n",
    "    for indexClass in range(0, classLen):\n",
    "        className = classList[indexClass]\n",
    "        result[className] = 0\n",
    "        if maxIndex == indexClass:\n",
    "            result[className] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1 : build feature\n",
      "step2: build training dataset from feature\n",
      "step3: make crossValidate on training\n",
      "accuracy  0.7625\n",
      "accuracy  0.7775\n",
      "accuracy  0.7375\n",
      "accuracy  0.7725\n",
      "accuracy  0.725\n",
      "accuracy  0.75\n",
      "accuracy  0.7875\n",
      "accuracy  0.7725\n",
      "accuracy  0.7725\n",
      "accuracy  0.745\n",
      "step4: retrieve mean accuracy and best model\n",
      "meanAccuracy  0.76025\n",
      "step5: make prediction for test data\n",
      "step6: output the prediction and write to file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main program MNBC\n",
    "import pandas as pd\n",
    "\n",
    "def mnbc_train_model(filePath):\n",
    "    seed()\n",
    "    # build training dataset base on the csv training file\n",
    "    text_df = pd.read_csv(filePath)\n",
    "\n",
    "    print(\"step1 : build feature\")\n",
    "    featureList = get1000MostFrequencyFeatureFrom(text_df[CONSTANT_LABEL_ABSTRACT])\n",
    "    #featureList = retrieveFeatureFrom(text_df[\"abstract\"])\n",
    "\n",
    "    print(\"step2: build training dataset from feature\")\n",
    "    featureDataSet = buildTrainingDataset(featureList, text_df[CONSTANT_LABEL_ABSTRACT])\n",
    "    classDataSet = buildClass(text_df[CONSTANT_LABEL_CLASS])\n",
    "    trainingDataSet = {**featureDataSet, **classDataSet}\n",
    "    trainingDF = pd.DataFrame(trainingDataSet)\n",
    "\n",
    "    print(\"step3: make crossValidate on training\")\n",
    "    results = kCrossValidate(trainingDF, classNameList, 10)\n",
    "\n",
    "    print(\"step4: retrieve mean accuracy and best model\")\n",
    "    meanAccuracy = retrieve_mean_accuracy(results)\n",
    "    #bestModel = retrieve_model_have_max_accuracy(results)\n",
    "\n",
    "    print(\"meanAccuracy \", meanAccuracy)\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"featureList\": featureList\n",
    "    }\n",
    "\n",
    "def mnbc_predict(testFile, outputFile, model, featureList):\n",
    "    print(\"step5: make prediction for test data\")\n",
    "    #make prediction for testing data\n",
    "    testData = pd.read_csv(testFile)\n",
    "    testDataSet = buildTestingDataset(featureList, testData[CONSTANT_LABEL_ABSTRACT])\n",
    "    testFeatureDF = pd.DataFrame(testDataSet)\n",
    "    predictionDF = predictMNBC(testFeatureDF, classNameList, model)\n",
    "\n",
    "    print(\"step6: output the prediction and write to file\")\n",
    "    outputDF = generate_prediction_output(predictionDF)\n",
    "    outputDF.to_csv(outputFile, index=False)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "train_models = mnbc_train_model(\"./trg.csv\")\n",
    "bestModel = retrieve_model_have_max_accuracy(train_models[\"results\"])\n",
    "mnbc_predict(\"./tst.csv\", \"./output/output.csv\", bestModel, train_models[\"featureList\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1 : build feature\n",
      "step2: build training dataset from feature\n",
      "step3: make crossValidate on training\n",
      "accuracy  0.8175675675675675\n",
      "accuracy  0.8018018018018018\n",
      "accuracy  0.8063063063063063\n",
      "accuracy  0.8220720720720721\n",
      "accuracy  0.8220720720720721\n",
      "accuracy  0.8108108108108109\n",
      "accuracy  0.8175675675675675\n",
      "accuracy  0.8175675675675675\n",
      "accuracy  0.8063063063063063\n",
      "step4: retrieve mean accuracy\n",
      "averageAccuracy  0.8135635635635635\n"
     ]
    }
   ],
   "source": [
    "# main program\n",
    "import pandas as pd\n",
    "\n",
    "# build training dataset base on the csv training file\n",
    "text_df = pd.read_csv(\"./trg.csv\")\n",
    "\n",
    "print(\"step1 : build feature\")\n",
    "featureList = get1000MostFrequencyFeatureFrom(text_df[CONSTANT_LABEL_ABSTRACT])\n",
    "#featureList = retrieveFeatureFrom(text_df[\"abstract\"])\n",
    "\n",
    "print(\"step2: build training dataset from feature\")\n",
    "featureDataSet = buildTrainingDataset(featureList, text_df[CONSTANT_LABEL_ABSTRACT])\n",
    "classDataSet = buildClass(text_df[CONSTANT_LABEL_CLASS])\n",
    "trainingDataSet = {**featureDataSet, **classDataSet}\n",
    "trainingDF = pd.DataFrame(trainingDataSet)\n",
    "\n",
    "print(\"step3: make crossValidate on training\")\n",
    "results = kCrossValidate(trainingDF, classNameList, 10)\n",
    "\n",
    "print(\"step4: retrieve mean accuracy and best model\")\n",
    "meanAccuracy = retrieve_mean_accuracy(results)\n",
    "bestModel = retrieve_model_have_max_accuracy(results)\n",
    "\n",
    "print(\"meanAccuracy \", meanAccuracy)\n",
    "\n",
    "print(\"step5: make prediction for test data\")\n",
    "#make prediction for testing data\n",
    "testData = pd.read_csv(\"./tst.csv\")\n",
    "testDataSet = buildTestingDataset(featureList, testData[CONSTANT_LABEL_ABSTRACT])\n",
    "testFeatureDF = pd.DataFrame(testDataSet)\n",
    "predictionDF = predict(testFeatureDF, classNameList, bestModel)\n",
    "\n",
    "print(\"step6: output the prediction and write to file\")\n",
    "outputDF = generate_prediction_output(predictionDF)\n",
    "outputDF.to_csv(\"./output/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-398-74c43d3da9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# sort, remove duplicated value in list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "# small tip to work with python\n",
    "\n",
    "\"\"\"\n",
    "work with list\n",
    "\"\"\"\n",
    "# find object in list\n",
    "fruits = ['apple', 'banana', 'cherry']\n",
    "found = not (\"cherry1\" in fruits)\n",
    "found # found is TRUE FALSE\n",
    "\n",
    "# map list \n",
    "my_string = \"blah, lots  ,  of ,  spaces, here \"\n",
    "result = [x.strip() for x in my_string.split(',')]\n",
    "result # remove all white space in the front and back.\n",
    "\n",
    "# sort, remove duplicated value in list\n",
    "words = list(np.unique(np.sort(words)))\n",
    "\n",
    "\"\"\"\n",
    "work with file\n",
    "\"\"\"\n",
    "# dataframe to csv file\n",
    "text_df.to_csv(\"/Users/mac/Desktop/NZ/UOA/COMPSCI_361/assignments/assignment_05/test.csv\", index=False) #not include row name\n",
    "text_df1 = pd.read_csv(\"./trg_copy.csv\") # read csv file\n",
    "\n",
    "# find most 100 frequency words\n",
    "occurenceList = Counter(words)\n",
    "most_1000_frequency = occurenceList.most_common(100)\n",
    "\n",
    "\"\"\"\n",
    "work with dictionary\n",
    "\"\"\"\n",
    "car = {\n",
    "  \"brand\": \"Ford\",\n",
    "  \"model\": \"Mustang\",\n",
    "  \"year\": 1964\n",
    "}\n",
    "\n",
    "x = car.items() # return a list of Tuple object from dictionary\n",
    "\n",
    "\"\"\"\n",
    "work with list\n",
    "\"\"\"\n",
    "\n",
    "#using map \n",
    "arr = [\"zclass_A\", \"zclass_B\", \"zclass_E\", \"zclass_V\"]\n",
    "l = list(map(lambda x: [x + \"=0\", x + \"=1\"], arr))\n",
    "\n",
    "#using flat function\n",
    "flat_list = [item for sublist in l for item in sublist]\n",
    "flat_list\n",
    "\n",
    "\"\"\"\n",
    "work with panda\n",
    "\"\"\"\n",
    "print(text_df.shape) # get dimension of dataframe \n",
    "print(text_df.loc[0]) # get Series object for certain row by index\n",
    "print(text_df.loc[\"a\"]) # get Series object for certain row by name\n",
    "print(text_df.loc[0].index.tolist()) # retrieve columns name list by Series object\n",
    "print(text_df[\"aa\"]) # get Series object for certain columns by name\n",
    "text_df[\"aa\"][0] = 1 # assign data for columns \"aa\", row 0 \n",
    "df_1 = pd.DataFrame(result , columns = columnsNameList, index=indexList) # create dataframe with result content, columns name list and rowsList\n",
    "\n",
    "# drop dataframe from list of columns or rows, axis = 1 indicate drop by columns, = 0 indicate drop by rows \n",
    "training_df.drop(classNameList, axis=1)\n",
    "\n",
    "# filter data frame using condition of columns\n",
    "#filterElm = (training_df[\"zclass_A\"] == 1) & (training_df[\"HERE\"] == 0)\n",
    "filterElm = (training_df[\"zclass_B\"] == 1)\n",
    "filter_df = training_df[filterElm]\n",
    "\n",
    "# calculate sum of all data in dataFrame\n",
    "a = model.sum().sum()\n",
    "a = model.sum() # return a list of sum for each columns\n",
    "\n",
    "np.zeros((3, 2), dtype = int) # create 3 horizontal;2 vertical dimension array with 0 default value \n",
    "\n",
    "\"\"\"\n",
    "Calculate for MNBC\n",
    "\"\"\"\n",
    "totalNo = model.loc[\"total\"].sum()\n",
    "noClass = model[\"zclass_B\"][\"total\"]\n",
    "cocurence = model[\"zclass_B\"].sum() - noClass\n",
    "\n",
    "print(\"totalNo\", totalNo)\n",
    "print(\"noClass\", noClass)\n",
    "print(\"cocurence Aa \", cocurence)\n",
    "\n",
    "\"\"\"\n",
    "Cross validation\n",
    "\"\"\"\n",
    "from random import seed\n",
    "from random import randrange\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    " \n",
    "# test cross validation split\n",
    "#seed(1)\n",
    "#dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "#folds = cross_validation_split(dataset, 3)\n",
    "#print(folds)\n",
    "\n",
    "\n",
    "data = range(10)\n",
    "\n",
    "folds = cross_validation_split(list(data), 3)\n",
    "print(len(folds))\n",
    "print(len(folds[0]))\n",
    "print(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
